#! /usr/bin/env python3

import argparse
from util import get_parser_file_type
from uniref_dataset import UnirefToSqliteParser, parse_go_annotations_meta, create_h5_dataset

if __name__ == '__main__':

    parser = argparse.ArgumentParser(description = 'Create an h5 dataset from a raw UniRef file through an intermediate sqlite DB.')
    parser.add_argument('--uniref-xml-gz-file', dest = 'uniref_xml_gz_file', metavar = '/path/to/unirefXX.xml.gz', type = get_parser_file_type(parser, must_exist = True), \
            required = True, help = 'Path to the raw UniRef file.')
    parser.add_argument('--go-annotations-meta-file', dest = 'go_annotations_meta_file', metavar = '/path/to/go.txt', type = get_parser_file_type(parser, must_exist = True), \
            required = True, help = 'Path to the specification file of all possible GO annotations (from CAFA).')
    parser.add_argument('--intermediate-sqlite-db-file', dest = 'intermediate_sqlite_db_file', metavar = '/path/to/uniref.db', type = get_parser_file_type(parser), \
            required = True, help = 'Path to the save the output sqlite file.')
    parser.add_argument('--intermediate-go-annotations-meta-csv-file', dest = 'intermediate_go_annotations_meta_csv_file', metavar = '/path/to/go_annotations.csv', \
            type = get_parser_file_type(parser), required = True, help = 'Path to the save the output CSV file with metadata for all the GO annotations.')
    parser.add_argument('--output-h5-dataset-file', dest = 'output_h5_dataset_file', metavar = '/path/to/dataset.h5', type = get_parser_file_type(parser), required = True, \
            help = 'Path to the save the output h5 dataset file.')
    parser.add_argument('--min-records-to-keep-annotation', dest = 'min_records_to_keep_annotation', metavar = '100', type = int, default = 100, help = 'The minimal number of ' + \
            'records required to encode an annotaiton (default 100).')
    parser.add_argument('--log-progress-every', dest = 'log_progress_every', metavar = '1000', type = int, default = 1000, help = 'If running in verbose (non-silent) mode, ' + \
            'log the progress of the process in increments of this given number (1000 by default).')
    parser.add_argument('--chunk-size', dest = 'chunk_size', metavar = '100000', type = int, default = 100000, help = 'The number of records processed per chunk.')
    parser.add_argument('--no-shuffle', dest = 'no_shuffle', action = 'store_true', help = 'By default, records are shuffled (to ensure a heterogenous dataset that trains ' + \
            'better). Provide this flag to disable that.')
    parser.add_argument('--records-limit', dest = 'records_limit', metavar = 'n', type = int, default = None, help = 'Limit the number of loaded records. By default will ' + \
            'load all records')
    parser.add_argument('--silent', dest = 'silent', action = 'store_true', help = 'Run in silent mode.')

    args = parser.parse_args()
    
    go_annotations_meta = parse_go_annotations_meta(args.go_annotations_meta_file)
    UnirefToSqliteParser(args.uniref_xml_gz_file, go_annotations_meta, args.intermediate_sqlite_db_file, verbose = not args.silent, log_progress_every = args.log_progress_every, \
            chunk_size = args.chunk_size).parse()
    go_annotations_meta.to_csv(args.intermediate_go_annotations_meta_csv_file)
    
    create_h5_dataset(args.intermediate_sqlite_db_file, args.intermediate_go_annotations_meta_csv_file, args.output_h5_dataset_file, shuffle = \
            not args.no_shuffle, min_records_to_keep_annotation = args.min_records_to_keep_annotation, records_limit = args.records_limit, save_chunk_size = args.chunk_size, \
            verbose = not args.silent)